{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loasklearn.linear_modelimport pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Bayes opt\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Linear Regression with Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "# Linear Regression with L2\n",
    "from sklearn.linear_model import Ridge\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "# LightGBM\n",
    "import lightgbm as lgbm\n",
    "\n",
    "# Scoring\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = pd.read_csv(\"../data/processed/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>internet</th>\n",
       "      <th>romantic</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>M</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
       "0     GP   F   18       U     GT3       A     4     4   at_home   teacher   \n",
       "1     GP   F   17       U     GT3       T     1     1   at_home     other   \n",
       "2     GP   F   15       U     GT3       T     4     2    health  services   \n",
       "3     GP   F   16       U     GT3       T     3     3     other     other   \n",
       "4     GP   M   16       U     LE3       T     4     3  services     other   \n",
       "\n",
       "   ... internet romantic  famrel  freetime  goout Dalc Walc health absences  \\\n",
       "0  ...       no       no       4         3      4    1    1      3        4   \n",
       "1  ...      yes       no       5         3      3    1    1      3        2   \n",
       "2  ...      yes      yes       3         2      2    1    1      5        0   \n",
       "3  ...       no       no       4         3      2    1    2      5        0   \n",
       "4  ...      yes       no       5         4      2    1    2      5        6   \n",
       "\n",
       "   G3  \n",
       "0  11  \n",
       "1  11  \n",
       "2  14  \n",
       "3  13  \n",
       "4  13  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop([\"G3\"], axis = 1)\n",
    "y_train = train_data[\"G3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu',\n",
       "       'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
       "       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
       "       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc',\n",
       "       'Walc', 'health', 'absences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining transformation steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    # For standard scaling of data\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    # OHE for categorical data\n",
    "    ('onehot', OneHotEncoder(drop = \"first\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical vs categorical features\n",
    "categorical_features = X_train.loc[:,(\"school\",\"sex\", \"address\", \"famsize\", \"Pstatus\", \"Mjob\", \"Fjob\", \"reason\", \n",
    "                                  \"guardian\",\"schoolsup\", \"famsup\", \"paid\",\"activities\",\"nursery\", \"higher\", \n",
    "                                  \"internet\",\"romantic\")].columns\n",
    "\n",
    "numeric_features = X_train.loc[:,(\"age\", \"Medu\", \"Fedu\", \"traveltime\", \"studytime\", \"failures\", \"famrel\", \n",
    "                                    \"freetime\", \"goout\", \"Dalc\", \"Walc\", \"health\", \"absences\")].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob',\n",
       "       'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities',\n",
       "       'nursery', 'higher', 'internet', 'romantic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel',\n",
       "       'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('ohe', OneHotEncoder(drop = \"first\"), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "X_train_trans = pd.DataFrame(preprocessor.fit_transform(X_train),\n",
    "                            index = X_train.index,\n",
    "                             columns = (list(numeric_features) +\n",
    "                                       list(preprocessor.named_transformers_['ohe'].get_feature_names(categorical_features))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmridge_params = {'alpha':(0,100)}\n",
    "\n",
    "def cv_mse_lmridge(alpha):\n",
    "    \"\"\" \n",
    "    Performs cross validation for LM regressor with Ridge regression. To be used for Bayesian optimiser maximizer function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "        L2 regularisation constant\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross validation score based on negative mean squared error.\n",
    "        \n",
    "    \"\"\"\n",
    "    estimator = Ridge(alpha)\n",
    "\n",
    "    # Note that neg_mean_squared_error is opposite, thus a negative sign is added for Minimisation Optimisation via BayesOpt\n",
    "    return cross_validate(estimator, X_train_trans, y_train, cv = 10, scoring = \"neg_root_mean_squared_error\")[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 72.45   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.746   \u001b[0m | \u001b[0m 13.06   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.737   \u001b[0m | \u001b[0m 94.96   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-2.735   \u001b[0m | \u001b[95m 55.57   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-2.739   \u001b[0m | \u001b[0m 28.27   \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-2.735   \u001b[0m | \u001b[95m 63.32   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 63.32   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m-2.735   \u001b[0m | \u001b[95m 61.72   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 68.61   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 55.83   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 70.74   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 56.32   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 70.37   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 56.45   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 57.6    \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 71.25   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 57.41   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.737   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 70.32   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 56.55   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 69.56   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 56.59   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 68.63   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 56.27   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 56.52   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "optimizer_lmridge = BayesianOptimization(cv_mse_lmridge, lmridge_params)\n",
    "optimizer_lmridge.maximize(n_iter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': -2.73504227729081, 'params': {'alpha': 61.716008089193764}}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_lmridge.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmlasso_params = {'alpha':(0,100)}\n",
    "\n",
    "def cv_mse_lmlasso(alpha):\n",
    "    \"\"\" \n",
    "    Performs cross validation for LM regressor with Lasso regression. To be used for Bayesian optimiser maximizer function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "        L1 regularisation constant\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross validation score based on negative mean squared error.\n",
    "        \n",
    "    \"\"\"\n",
    "    estimator = Lasso(alpha)\n",
    "\n",
    "    # Note that neg_mean_squared_error is opposite, thus a negative sign is added for Minimisation Optimisation via BayesOpt\n",
    "    return cross_validate(estimator, X_train_trans, y_train, cv = 10, scoring = \"neg_root_mean_squared_error\")[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   alpha   |\n",
      "-------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 70.12   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 85.44   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 31.35   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 46.36   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 62.58   \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-2.757   \u001b[0m | \u001b[95m 0.003969\u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1734.3718561958653, tolerance: 0.5434673076923078\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1654.4138581748339, tolerance: 0.539591452991453\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1676.8039495252024, tolerance: 0.5247307692307692\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1679.949379948897, tolerance: 0.5392536324786326\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1646.2405044878421, tolerance: 0.5244923076923077\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1668.2058533634345, tolerance: 0.5121299145299145\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1578.8338587129879, tolerance: 0.5062307692307692\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1667.2704422013057, tolerance: 0.5325692307692309\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1471.7341993602565, tolerance: 0.4718222222222223\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:515: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/home/kenneth/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1215.2154291436132, tolerance: 0.40789465811965814\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.77    \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.98    \u001b[0m | \u001b[0m 0.638   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 19.45   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 93.82   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 54.56   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 38.55   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 9.394   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 78.23   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 25.4    \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 14.42   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 5.097   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 89.63   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 50.46   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 74.17   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 58.57   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 42.45   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 66.35   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-3.2     \u001b[0m | \u001b[0m 81.83   \u001b[0m |\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "optimizer_lmlasso = BayesianOptimization(cv_mse_lmlasso, lmlasso_params, random_state = 1)\n",
    "optimizer_lmlasso.maximize(n_iter = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn Random Forest\n",
    "rf_params = {'n_estimators':(10,150), 'max_depth':(10,200), 'max_features':(2, 30)}\n",
    "\n",
    "def cv_mse_rf(n_estimators,max_depth, max_features):\n",
    "    \"\"\" \n",
    "    Performs cross validation for Random Forest Regressor. To be used for Bayesian optimiser maximizer function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : float\n",
    "        Number of estimators for random forest\n",
    "    max_depth : float\n",
    "        Max depth of trees in random forest\n",
    "    max_features : float\n",
    "        Max number of features in random forest\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross validation score based on negative mean squared error.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Convert chosen hyperparams to discrete integer\n",
    "    max_depth = int(max_depth)\n",
    "    max_features = int(max_features)\n",
    "    n_estimators = int(n_estimators)\n",
    "    \n",
    "    estimator = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features)\n",
    "\n",
    "    # Note that neg_mean_squared_error is opposite, thus a negative sign is added for Minimisation Optimisation via BayesOpt\n",
    "    return cross_validate(estimator, X_train_trans, y_train, cv = 10, scoring = \"neg_root_mean_squared_error\")[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | max_fe... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.714   \u001b[0m | \u001b[0m 14.78   \u001b[0m | \u001b[0m 21.4    \u001b[0m | \u001b[0m 41.23   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.717   \u001b[0m | \u001b[0m 15.92   \u001b[0m | \u001b[0m 20.51   \u001b[0m | \u001b[0m 36.09   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-2.688   \u001b[0m | \u001b[95m 59.09   \u001b[0m | \u001b[95m 11.31   \u001b[0m | \u001b[95m 65.41   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-2.711   \u001b[0m | \u001b[0m 72.22   \u001b[0m | \u001b[0m 24.64   \u001b[0m | \u001b[0m 80.11   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-2.693   \u001b[0m | \u001b[0m 88.84   \u001b[0m | \u001b[0m 19.55   \u001b[0m | \u001b[0m 62.83   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-2.729   \u001b[0m | \u001b[0m 11.14   \u001b[0m | \u001b[0m 2.375   \u001b[0m | \u001b[0m 149.3   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.704   \u001b[0m | \u001b[0m 99.85   \u001b[0m | \u001b[0m 2.073   \u001b[0m | \u001b[0m 148.1   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.837   \u001b[0m | \u001b[0m 97.81   \u001b[0m | \u001b[0m 2.188   \u001b[0m | \u001b[0m 12.58   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.748   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 90.51   \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m-2.683   \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 30.0    \u001b[0m | \u001b[95m 150.0   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.738   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 69.23   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.716   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 150.0   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.747   \u001b[0m | \u001b[0m 62.54   \u001b[0m | \u001b[0m 29.89   \u001b[0m | \u001b[0m 29.18   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.693   \u001b[0m | \u001b[0m 57.42   \u001b[0m | \u001b[0m 26.52   \u001b[0m | \u001b[0m 149.8   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.708   \u001b[0m | \u001b[0m 48.38   \u001b[0m | \u001b[0m 2.008   \u001b[0m | \u001b[0m 41.62   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.725   \u001b[0m | \u001b[0m 65.18   \u001b[0m | \u001b[0m 2.939   \u001b[0m | \u001b[0m 149.5   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.712   \u001b[0m | \u001b[0m 99.72   \u001b[0m | \u001b[0m 29.44   \u001b[0m | \u001b[0m 59.99   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.718   \u001b[0m | \u001b[0m 83.44   \u001b[0m | \u001b[0m 29.28   \u001b[0m | \u001b[0m 149.8   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.724   \u001b[0m | \u001b[0m 24.35   \u001b[0m | \u001b[0m 29.66   \u001b[0m | \u001b[0m 112.0   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.707   \u001b[0m | \u001b[0m 36.56   \u001b[0m | \u001b[0m 29.32   \u001b[0m | \u001b[0m 149.9   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.802   \u001b[0m | \u001b[0m 10.03   \u001b[0m | \u001b[0m 2.113   \u001b[0m | \u001b[0m 10.99   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.704   \u001b[0m | \u001b[0m 41.46   \u001b[0m | \u001b[0m 29.69   \u001b[0m | \u001b[0m 61.64   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.741   \u001b[0m | \u001b[0m 81.76   \u001b[0m | \u001b[0m 2.0     \u001b[0m | \u001b[0m 53.75   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.705   \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 24.99   \u001b[0m | \u001b[0m 124.6   \u001b[0m |\n",
      "| \u001b[95m 25      \u001b[0m | \u001b[95m-2.676   \u001b[0m | \u001b[95m 45.28   \u001b[0m | \u001b[95m 13.7    \u001b[0m | \u001b[95m 127.5   \u001b[0m |\n",
      "| \u001b[95m 26      \u001b[0m | \u001b[95m-2.666   \u001b[0m | \u001b[95m 37.51   \u001b[0m | \u001b[95m 8.723   \u001b[0m | \u001b[95m 98.37   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-2.703   \u001b[0m | \u001b[0m 25.33   \u001b[0m | \u001b[0m 2.038   \u001b[0m | \u001b[0m 114.6   \u001b[0m |\n",
      "| \u001b[95m 28      \u001b[0m | \u001b[95m-2.659   \u001b[0m | \u001b[95m 33.57   \u001b[0m | \u001b[95m 13.1    \u001b[0m | \u001b[95m 75.3    \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-2.691   \u001b[0m | \u001b[0m 32.79   \u001b[0m | \u001b[0m 12.54   \u001b[0m | \u001b[0m 57.96   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.883   \u001b[0m | \u001b[0m 10.72   \u001b[0m | \u001b[0m 29.15   \u001b[0m | \u001b[0m 10.3    \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-2.663   \u001b[0m | \u001b[0m 20.8    \u001b[0m | \u001b[0m 18.38   \u001b[0m | \u001b[0m 86.34   \u001b[0m |\n",
      "| \u001b[95m 32      \u001b[0m | \u001b[95m-2.65    \u001b[0m | \u001b[95m 36.88   \u001b[0m | \u001b[95m 19.26   \u001b[0m | \u001b[95m 86.48   \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-2.924   \u001b[0m | \u001b[0m 48.74   \u001b[0m | \u001b[0m 2.546   \u001b[0m | \u001b[0m 10.4    \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-2.812   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-2.722   \u001b[0m | \u001b[0m 99.62   \u001b[0m | \u001b[0m 18.98   \u001b[0m | \u001b[0m 38.86   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-2.656   \u001b[0m | \u001b[0m 31.34   \u001b[0m | \u001b[0m 13.51   \u001b[0m | \u001b[0m 89.1    \u001b[0m |\n",
      "| \u001b[95m 37      \u001b[0m | \u001b[95m-2.627   \u001b[0m | \u001b[95m 63.73   \u001b[0m | \u001b[95m 7.819   \u001b[0m | \u001b[95m 114.2   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-2.724   \u001b[0m | \u001b[0m 76.2    \u001b[0m | \u001b[0m 2.328   \u001b[0m | \u001b[0m 120.7   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-2.708   \u001b[0m | \u001b[0m 62.89   \u001b[0m | \u001b[0m 23.9    \u001b[0m | \u001b[0m 113.2   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-2.712   \u001b[0m | \u001b[0m 61.33   \u001b[0m | \u001b[0m 2.832   \u001b[0m | \u001b[0m 98.52   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-2.653   \u001b[0m | \u001b[0m 10.09   \u001b[0m | \u001b[0m 16.78   \u001b[0m | \u001b[0m 115.1   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-2.685   \u001b[0m | \u001b[0m 99.86   \u001b[0m | \u001b[0m 25.1    \u001b[0m | \u001b[0m 91.58   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-2.724   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 97.66   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-2.672   \u001b[0m | \u001b[0m 98.91   \u001b[0m | \u001b[0m 16.47   \u001b[0m | \u001b[0m 149.7   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-2.683   \u001b[0m | \u001b[0m 55.5    \u001b[0m | \u001b[0m 6.816   \u001b[0m | \u001b[0m 116.7   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-2.684   \u001b[0m | \u001b[0m 79.05   \u001b[0m | \u001b[0m 13.84   \u001b[0m | \u001b[0m 105.4   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.677   \u001b[0m | \u001b[0m 19.93   \u001b[0m | \u001b[0m 15.85   \u001b[0m | \u001b[0m 137.2   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-2.73    \u001b[0m | \u001b[0m 10.38   \u001b[0m | \u001b[0m 3.149   \u001b[0m | \u001b[0m 37.59   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-2.651   \u001b[0m | \u001b[0m 40.72   \u001b[0m | \u001b[0m 11.63   \u001b[0m | \u001b[0m 150.0   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-2.692   \u001b[0m | \u001b[0m 10.54   \u001b[0m | \u001b[0m 3.679   \u001b[0m | \u001b[0m 101.3   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-2.678   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 3.884   \u001b[0m | \u001b[0m 60.62   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-2.69    \u001b[0m | \u001b[0m 67.7    \u001b[0m | \u001b[0m 14.48   \u001b[0m | \u001b[0m 127.2   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-2.71    \u001b[0m | \u001b[0m 58.65   \u001b[0m | \u001b[0m 17.7    \u001b[0m | \u001b[0m 45.48   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-2.672   \u001b[0m | \u001b[0m 62.77   \u001b[0m | \u001b[0m 13.07   \u001b[0m | \u001b[0m 106.5   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-2.742   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 70.14   \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer_rf = BayesianOptimization(cv_mse_rf, rf_params)\n",
    "optimizer_rf.maximize(n_iter = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn Random Forest\n",
    "xgb_params = {'n_estimators':(10, 150), 'max_depth':(10, 200), 'learning_rate':(0, 1),\n",
    "              'subsample':(0, 1), 'gamma':(0, 50), 'reg_alpha':(0, 100), 'reg_lambda':(0, 100)}\n",
    "\n",
    "def cv_mse_xgb(n_estimators, max_depth, learning_rate, subsample, gamma, reg_alpha, reg_lambda):\n",
    "    \"\"\" \n",
    "    Performs cross validation for Random Forest Regressor. To be used for Bayesian optimiser maximizer function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : float\n",
    "        Number of estimators\n",
    "    max_depth : float\n",
    "        Max depth of trees\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    subsample : float\n",
    "        Subsample ratio of training instances \n",
    "    gamma : float\n",
    "        Min loss reduction to make further partition on leaf node   \n",
    "    reg_alpha : float\n",
    "        L1 regularisation\n",
    "    reg_lambda : float\n",
    "        L2 regularisation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross validation score based on negative mean squared error.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Convert chosen hyperparams to discrete integer\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    \n",
    "    estimator = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                 n_estimators = n_estimators, \n",
    "                                 max_depth = max_depth, \n",
    "                                 learning_rate = learning_rate, \n",
    "                                 subsample = subsample,\n",
    "                                 gamma = gamma, \n",
    "                                 reg_alpha = reg_alpha, \n",
    "                                 reg_lambda = reg_lambda)\n",
    "\n",
    "    # Note that neg_mean_squared_error is opposite, thus a negative sign is added for Minimisation Optimisation via BayesOpt\n",
    "    return cross_validate(estimator, X_train_trans, y_train, cv = 10, scoring = \"neg_root_mean_squared_error\")[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   gamma   | learni... | max_depth | n_esti... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.909   \u001b[0m | \u001b[0m 20.85   \u001b[0m | \u001b[0m 0.7203  \u001b[0m | \u001b[0m 10.01   \u001b[0m | \u001b[0m 52.33   \u001b[0m | \u001b[0m 14.68   \u001b[0m | \u001b[0m 9.234   \u001b[0m | \u001b[0m 0.1863  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-2.729   \u001b[0m | \u001b[95m 17.28   \u001b[0m | \u001b[95m 0.3968  \u001b[0m | \u001b[95m 58.49   \u001b[0m | \u001b[95m 68.69   \u001b[0m | \u001b[95m 68.52   \u001b[0m | \u001b[95m 20.45   \u001b[0m | \u001b[95m 0.8781  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.961   \u001b[0m | \u001b[0m 1.369   \u001b[0m | \u001b[0m 0.6705  \u001b[0m | \u001b[0m 47.56   \u001b[0m | \u001b[0m 88.22   \u001b[0m | \u001b[0m 14.04   \u001b[0m | \u001b[0m 19.81   \u001b[0m | \u001b[0m 0.8007  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-4.359   \u001b[0m | \u001b[0m 48.41   \u001b[0m | \u001b[0m 0.3134  \u001b[0m | \u001b[0m 72.31   \u001b[0m | \u001b[0m 132.7   \u001b[0m | \u001b[0m 89.46   \u001b[0m | \u001b[0m 8.504   \u001b[0m | \u001b[0m 0.03905 \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-2.728   \u001b[0m | \u001b[95m 8.492   \u001b[0m | \u001b[95m 0.8781  \u001b[0m | \u001b[95m 18.85   \u001b[0m | \u001b[95m 68.96   \u001b[0m | \u001b[95m 95.79   \u001b[0m | \u001b[95m 53.32   \u001b[0m | \u001b[95m 0.6919  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-11.79   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 42.16   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.744   \u001b[0m | \u001b[0m 11.43   \u001b[0m | \u001b[0m 0.8313  \u001b[0m | \u001b[0m 19.31   \u001b[0m | \u001b[0m 76.88   \u001b[0m | \u001b[0m 57.02   \u001b[0m | \u001b[0m 23.01   \u001b[0m | \u001b[0m 0.5994  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-2.86    \u001b[0m | \u001b[0m 35.33   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 42.07   \u001b[0m | \u001b[0m 71.69   \u001b[0m | \u001b[0m 37.13   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.838   \u001b[0m | \u001b[0m 21.76   \u001b[0m | \u001b[0m 0.8607  \u001b[0m | \u001b[0m 23.27   \u001b[0m | \u001b[0m 52.18   \u001b[0m | \u001b[0m 94.1    \u001b[0m | \u001b[0m 8.591   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-11.79   \u001b[0m | \u001b[0m 50.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 16.89   \u001b[0m | \u001b[0m 68.03   \u001b[0m | \u001b[0m 75.62   \u001b[0m | \u001b[0m 40.3    \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.755   \u001b[0m | \u001b[0m 16.45   \u001b[0m | \u001b[0m 0.5496  \u001b[0m | \u001b[0m 49.05   \u001b[0m | \u001b[0m 70.31   \u001b[0m | \u001b[0m 64.43   \u001b[0m | \u001b[0m 19.09   \u001b[0m | \u001b[0m 0.8479  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.903   \u001b[0m | \u001b[0m 17.05   \u001b[0m | \u001b[0m 0.8665  \u001b[0m | \u001b[0m 33.55   \u001b[0m | \u001b[0m 73.93   \u001b[0m | \u001b[0m 47.33   \u001b[0m | \u001b[0m 13.31   \u001b[0m | \u001b[0m 0.8076  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.792   \u001b[0m | \u001b[0m 7.747   \u001b[0m | \u001b[0m 0.9953  \u001b[0m | \u001b[0m 29.16   \u001b[0m | \u001b[0m 68.12   \u001b[0m | \u001b[0m 66.57   \u001b[0m | \u001b[0m 13.69   \u001b[0m | \u001b[0m 0.9602  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.82    \u001b[0m | \u001b[0m 0.986   \u001b[0m | \u001b[0m 0.8016  \u001b[0m | \u001b[0m 35.43   \u001b[0m | \u001b[0m 77.29   \u001b[0m | \u001b[0m 55.63   \u001b[0m | \u001b[0m 24.3    \u001b[0m | \u001b[0m 0.8092  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.928   \u001b[0m | \u001b[0m 18.07   \u001b[0m | \u001b[0m 0.8174  \u001b[0m | \u001b[0m 31.18   \u001b[0m | \u001b[0m 68.87   \u001b[0m | \u001b[0m 22.32   \u001b[0m | \u001b[0m 9.385   \u001b[0m | \u001b[0m 0.6456  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.808   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 27.98   \u001b[0m | \u001b[0m 67.39   \u001b[0m | \u001b[0m 86.37   \u001b[0m | \u001b[0m 32.38   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.819   \u001b[0m | \u001b[0m 8.348   \u001b[0m | \u001b[0m 0.8581  \u001b[0m | \u001b[0m 44.05   \u001b[0m | \u001b[0m 58.83   \u001b[0m | \u001b[0m 87.01   \u001b[0m | \u001b[0m 12.79   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-3.05    \u001b[0m | \u001b[0m 0.2777  \u001b[0m | \u001b[0m 0.9891  \u001b[0m | \u001b[0m 15.87   \u001b[0m | \u001b[0m 66.92   \u001b[0m | \u001b[0m 39.76   \u001b[0m | \u001b[0m 16.13   \u001b[0m | \u001b[0m 0.5968  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.838   \u001b[0m | \u001b[0m 13.09   \u001b[0m | \u001b[0m 0.7001  \u001b[0m | \u001b[0m 59.32   \u001b[0m | \u001b[0m 72.0    \u001b[0m | \u001b[0m 39.34   \u001b[0m | \u001b[0m 9.36    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.837   \u001b[0m | \u001b[0m 14.08   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 53.18   \u001b[0m | \u001b[0m 55.56   \u001b[0m | \u001b[0m 60.32   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-2.815   \u001b[0m | \u001b[0m 5.856   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 54.53   \u001b[0m | \u001b[0m 81.42   \u001b[0m | \u001b[0m 67.32   \u001b[0m | \u001b[0m 0.8495  \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.834   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 18.89   \u001b[0m | \u001b[0m 62.82   \u001b[0m | \u001b[0m 96.21   \u001b[0m | \u001b[0m 3.141   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.817   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 21.16   \u001b[0m | \u001b[0m 43.36   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 25.64   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.819   \u001b[0m | \u001b[0m 3.544   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 25.84   \u001b[0m | \u001b[0m 37.03   \u001b[0m | \u001b[0m 82.43   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.872   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 16.84   \u001b[0m | \u001b[0m 94.3    \u001b[0m | \u001b[0m 59.1    \u001b[0m | \u001b[0m 6.551   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.855   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 39.77   \u001b[0m | \u001b[0m 59.01   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 57.97   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-2.852   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 30.28   \u001b[0m | \u001b[0m 90.49   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 55.57   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.807   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 13.93   \u001b[0m | \u001b[0m 70.55   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 78.81   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-2.903   \u001b[0m | \u001b[0m 18.08   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 42.74   \u001b[0m | \u001b[0m 104.5   \u001b[0m | \u001b[0m 33.06   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-2.881   \u001b[0m | \u001b[0m 19.46   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 47.37   \u001b[0m | \u001b[0m 29.75   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-3.117   \u001b[0m | \u001b[0m 34.52   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 60.14   \u001b[0m | \u001b[0m 85.03   \u001b[0m | \u001b[0m 5.689   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-2.868   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 14.2    \u001b[0m | \u001b[0m 105.0   \u001b[0m | \u001b[0m 32.36   \u001b[0m | \u001b[0m 31.52   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-3.248   \u001b[0m | \u001b[0m 15.37   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 97.3    \u001b[0m | \u001b[0m 8.503   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-2.844   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 54.91   \u001b[0m | \u001b[0m 113.3   \u001b[0m | \u001b[0m 46.48   \u001b[0m | \u001b[0m 32.03   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-11.79   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 85.54   \u001b[0m | \u001b[0m 89.26   \u001b[0m | \u001b[0m 57.38   \u001b[0m | \u001b[0m 27.03   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-2.859   \u001b[0m | \u001b[0m 4.436   \u001b[0m | \u001b[0m 0.8842  \u001b[0m | \u001b[0m 34.78   \u001b[0m | \u001b[0m 102.5   \u001b[0m | \u001b[0m 41.88   \u001b[0m | \u001b[0m 20.92   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-11.79   \u001b[0m | \u001b[0m 1.577   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 40.14   \u001b[0m | \u001b[0m 82.0    \u001b[0m | \u001b[0m 39.48   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-2.851   \u001b[0m | \u001b[0m 12.73   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.14   \u001b[0m | \u001b[0m 72.63   \u001b[0m | \u001b[0m 57.84   \u001b[0m | \u001b[0m 21.05   \u001b[0m | \u001b[0m 0.7627  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-2.818   \u001b[0m | \u001b[0m 11.33   \u001b[0m | \u001b[0m 0.8998  \u001b[0m | \u001b[0m 52.09   \u001b[0m | \u001b[0m 67.22   \u001b[0m | \u001b[0m 69.14   \u001b[0m | \u001b[0m 8.348   \u001b[0m | \u001b[0m 0.9771  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-2.849   \u001b[0m | \u001b[0m 6.657   \u001b[0m | \u001b[0m 0.948   \u001b[0m | \u001b[0m 26.98   \u001b[0m | \u001b[0m 53.43   \u001b[0m | \u001b[0m 90.64   \u001b[0m | \u001b[0m 11.14   \u001b[0m | \u001b[0m 0.9976  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-2.825   \u001b[0m | \u001b[0m 17.24   \u001b[0m | \u001b[0m 0.9145  \u001b[0m | \u001b[0m 56.74   \u001b[0m | \u001b[0m 66.17   \u001b[0m | \u001b[0m 55.39   \u001b[0m | \u001b[0m 11.02   \u001b[0m | \u001b[0m 0.9747  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-2.796   \u001b[0m | \u001b[0m 6.577   \u001b[0m | \u001b[0m 0.8477  \u001b[0m | \u001b[0m 38.6    \u001b[0m | \u001b[0m 66.95   \u001b[0m | \u001b[0m 76.58   \u001b[0m | \u001b[0m 20.61   \u001b[0m | \u001b[0m 0.9341  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-2.781   \u001b[0m | \u001b[0m 2.466   \u001b[0m | \u001b[0m 0.911   \u001b[0m | \u001b[0m 26.98   \u001b[0m | \u001b[0m 75.52   \u001b[0m | \u001b[0m 68.73   \u001b[0m | \u001b[0m 25.27   \u001b[0m | \u001b[0m 0.8367  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-2.772   \u001b[0m | \u001b[0m 6.442   \u001b[0m | \u001b[0m 0.3925  \u001b[0m | \u001b[0m 33.94   \u001b[0m | \u001b[0m 101.3   \u001b[0m | \u001b[0m 40.88   \u001b[0m | \u001b[0m 22.22   \u001b[0m | \u001b[0m 0.8733  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-2.808   \u001b[0m | \u001b[0m 0.4478  \u001b[0m | \u001b[0m 0.994   \u001b[0m | \u001b[0m 26.95   \u001b[0m | \u001b[0m 72.77   \u001b[0m | \u001b[0m 99.07   \u001b[0m | \u001b[0m 62.12   \u001b[0m | \u001b[0m 0.9822  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-2.805   \u001b[0m | \u001b[0m 1.972   \u001b[0m | \u001b[0m 0.9718  \u001b[0m | \u001b[0m 31.7    \u001b[0m | \u001b[0m 71.37   \u001b[0m | \u001b[0m 95.77   \u001b[0m | \u001b[0m 47.14   \u001b[0m | \u001b[0m 0.9319  \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.8     \u001b[0m | \u001b[0m 26.29   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 45.91   \u001b[0m | \u001b[0m 67.06   \u001b[0m | \u001b[0m 43.25   \u001b[0m | \u001b[0m 14.83   \u001b[0m | \u001b[0m 0.8399  \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-2.855   \u001b[0m | \u001b[0m 5.209   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 23.64   \u001b[0m | \u001b[0m 92.58   \u001b[0m | \u001b[0m 53.59   \u001b[0m | \u001b[0m 26.1    \u001b[0m | \u001b[0m 0.872   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-2.818   \u001b[0m | \u001b[0m 25.26   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 28.06   \u001b[0m | \u001b[0m 66.41   \u001b[0m | \u001b[0m 38.01   \u001b[0m | \u001b[0m 17.02   \u001b[0m | \u001b[0m 0.6494  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-2.844   \u001b[0m | \u001b[0m 13.29   \u001b[0m | \u001b[0m 0.9759  \u001b[0m | \u001b[0m 36.0    \u001b[0m | \u001b[0m 52.54   \u001b[0m | \u001b[0m 74.66   \u001b[0m | \u001b[0m 6.688   \u001b[0m | \u001b[0m 0.9615  \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-2.795   \u001b[0m | \u001b[0m 14.33   \u001b[0m | \u001b[0m 0.9393  \u001b[0m | \u001b[0m 36.85   \u001b[0m | \u001b[0m 41.61   \u001b[0m | \u001b[0m 91.35   \u001b[0m | \u001b[0m 4.242   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-2.816   \u001b[0m | \u001b[0m 23.58   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 37.03   \u001b[0m | \u001b[0m 60.65   \u001b[0m | \u001b[0m 56.44   \u001b[0m | \u001b[0m 9.734   \u001b[0m | \u001b[0m 0.8929  \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-2.87    \u001b[0m | \u001b[0m 14.21   \u001b[0m | \u001b[0m 0.8937  \u001b[0m | \u001b[0m 49.37   \u001b[0m | \u001b[0m 53.11   \u001b[0m | \u001b[0m 66.3    \u001b[0m | \u001b[0m 18.63   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-2.83    \u001b[0m | \u001b[0m 2.926   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 34.23   \u001b[0m | \u001b[0m 108.0   \u001b[0m | \u001b[0m 42.66   \u001b[0m | \u001b[0m 36.12   \u001b[0m | \u001b[0m 0.907   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-2.785   \u001b[0m | \u001b[0m 25.57   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 62.86   \u001b[0m | \u001b[0m 63.81   \u001b[0m | \u001b[0m 37.36   \u001b[0m | \u001b[0m 8.196   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m-2.794   \u001b[0m | \u001b[0m 15.16   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 28.07   \u001b[0m | \u001b[0m 79.15   \u001b[0m | \u001b[0m 41.7    \u001b[0m | \u001b[0m 30.61   \u001b[0m | \u001b[0m 0.7596  \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m-2.857   \u001b[0m | \u001b[0m 15.06   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 42.17   \u001b[0m | \u001b[0m 113.5   \u001b[0m | \u001b[0m 35.86   \u001b[0m | \u001b[0m 22.12   \u001b[0m | \u001b[0m 0.9614  \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m-2.825   \u001b[0m | \u001b[0m 12.23   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 22.06   \u001b[0m | \u001b[0m 111.4   \u001b[0m | \u001b[0m 44.04   \u001b[0m | \u001b[0m 23.15   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m-2.856   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 23.31   \u001b[0m | \u001b[0m 54.13   \u001b[0m | \u001b[0m 98.13   \u001b[0m | \u001b[0m 44.46   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m-2.819   \u001b[0m | \u001b[0m 16.84   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 38.7    \u001b[0m | \u001b[0m 103.1   \u001b[0m | \u001b[0m 52.88   \u001b[0m | \u001b[0m 28.52   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m-2.769   \u001b[0m | \u001b[0m 30.15   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 34.68   \u001b[0m | \u001b[0m 84.92   \u001b[0m | \u001b[0m 44.82   \u001b[0m | \u001b[0m 20.03   \u001b[0m | \u001b[0m 0.933   \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m-2.832   \u001b[0m | \u001b[0m 30.8    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 43.99   \u001b[0m | \u001b[0m 76.19   \u001b[0m | \u001b[0m 24.55   \u001b[0m | \u001b[0m 18.75   \u001b[0m | \u001b[0m 0.7746  \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m-2.842   \u001b[0m | \u001b[0m 31.43   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 38.66   \u001b[0m | \u001b[0m 102.7   \u001b[0m | \u001b[0m 30.59   \u001b[0m | \u001b[0m 13.22   \u001b[0m | \u001b[0m 0.891   \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m-2.799   \u001b[0m | \u001b[0m 21.52   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 28.39   \u001b[0m | \u001b[0m 98.19   \u001b[0m | \u001b[0m 30.61   \u001b[0m | \u001b[0m 31.32   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m-2.811   \u001b[0m | \u001b[0m 9.769   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 40.99   \u001b[0m | \u001b[0m 81.2    \u001b[0m | \u001b[0m 82.23   \u001b[0m | \u001b[0m 5.748   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m-2.794   \u001b[0m | \u001b[0m 17.32   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 57.81   \u001b[0m | \u001b[0m 65.63   \u001b[0m | \u001b[0m 30.26   \u001b[0m | \u001b[0m 24.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m-2.799   \u001b[0m | \u001b[0m 20.92   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 47.56   \u001b[0m | \u001b[0m 89.76   \u001b[0m | \u001b[0m 33.38   \u001b[0m | \u001b[0m 32.07   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m-2.779   \u001b[0m | \u001b[0m 31.47   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 59.39   \u001b[0m | \u001b[0m 82.53   \u001b[0m | \u001b[0m 37.45   \u001b[0m | \u001b[0m 15.2    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m-2.81    \u001b[0m | \u001b[0m 2.788   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 16.5    \u001b[0m | \u001b[0m 82.07   \u001b[0m | \u001b[0m 79.31   \u001b[0m | \u001b[0m 9.569   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m-2.91    \u001b[0m | \u001b[0m 16.47   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.99   \u001b[0m | \u001b[0m 77.4    \u001b[0m | \u001b[0m 15.61   \u001b[0m | \u001b[0m 30.48   \u001b[0m | \u001b[0m 0.5214  \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m-2.913   \u001b[0m | \u001b[0m 20.57   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 46.37   \u001b[0m | \u001b[0m 99.16   \u001b[0m | \u001b[0m 13.39   \u001b[0m | \u001b[0m 22.99   \u001b[0m | \u001b[0m 0.8015  \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m-2.897   \u001b[0m | \u001b[0m 26.06   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 47.7    \u001b[0m | \u001b[0m 118.1   \u001b[0m | \u001b[0m 25.09   \u001b[0m | \u001b[0m 0.7514  \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m-2.788   \u001b[0m | \u001b[0m 27.15   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 54.66   \u001b[0m | \u001b[0m 78.3    \u001b[0m | \u001b[0m 72.51   \u001b[0m | \u001b[0m 1.843   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m-2.928   \u001b[0m | \u001b[0m 3.106   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 38.93   \u001b[0m | \u001b[0m 99.89   \u001b[0m | \u001b[0m 20.34   \u001b[0m | \u001b[0m 37.17   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m-2.78    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.51   \u001b[0m | \u001b[0m 58.73   \u001b[0m | \u001b[0m 74.75   \u001b[0m | \u001b[0m 15.99   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m-2.834   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 23.69   \u001b[0m | \u001b[0m 57.18   \u001b[0m | \u001b[0m 56.29   \u001b[0m | \u001b[0m 30.19   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m-11.79   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 29.39   \u001b[0m | \u001b[0m 42.43   \u001b[0m | \u001b[0m 76.73   \u001b[0m | \u001b[0m 25.99   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m-2.857   \u001b[0m | \u001b[0m 11.62   \u001b[0m | \u001b[0m 0.9288  \u001b[0m | \u001b[0m 29.28   \u001b[0m | \u001b[0m 66.85   \u001b[0m | \u001b[0m 84.41   \u001b[0m | \u001b[0m 6.392   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m-2.814   \u001b[0m | \u001b[0m 22.68   \u001b[0m | \u001b[0m 0.9581  \u001b[0m | \u001b[0m 43.82   \u001b[0m | \u001b[0m 64.21   \u001b[0m | \u001b[0m 76.92   \u001b[0m | \u001b[0m 10.19   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m-2.839   \u001b[0m | \u001b[0m 0.2773  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 21.71   \u001b[0m | \u001b[0m 71.03   \u001b[0m | \u001b[0m 50.67   \u001b[0m | \u001b[0m 32.3    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m-2.757   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 11.35   \u001b[0m | \u001b[0m 72.14   \u001b[0m | \u001b[0m 65.07   \u001b[0m | \u001b[0m 15.8    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m-2.817   \u001b[0m | \u001b[0m 7.413   \u001b[0m | \u001b[0m 0.9841  \u001b[0m | \u001b[0m 23.17   \u001b[0m | \u001b[0m 45.88   \u001b[0m | \u001b[0m 98.03   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m-2.854   \u001b[0m | \u001b[0m 4.941   \u001b[0m | \u001b[0m 0.9642  \u001b[0m | \u001b[0m 20.08   \u001b[0m | \u001b[0m 63.16   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 22.18   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m-2.854   \u001b[0m | \u001b[0m 2.693   \u001b[0m | \u001b[0m 0.991   \u001b[0m | \u001b[0m 30.78   \u001b[0m | \u001b[0m 77.55   \u001b[0m | \u001b[0m 88.77   \u001b[0m | \u001b[0m 19.83   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m-2.833   \u001b[0m | \u001b[0m 7.909   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 67.72   \u001b[0m | \u001b[0m 84.86   \u001b[0m | \u001b[0m 13.84   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m-2.805   \u001b[0m | \u001b[0m 11.45   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 35.93   \u001b[0m | \u001b[0m 58.16   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 3.668   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m-2.798   \u001b[0m | \u001b[0m 20.41   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 45.85   \u001b[0m | \u001b[0m 74.37   \u001b[0m | \u001b[0m 44.67   \u001b[0m | \u001b[0m 29.74   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m-2.8     \u001b[0m | \u001b[0m 10.94   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 30.73   \u001b[0m | \u001b[0m 60.42   \u001b[0m | \u001b[0m 43.77   \u001b[0m | \u001b[0m 27.39   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m-2.793   \u001b[0m | \u001b[0m 9.673   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 15.41   \u001b[0m | \u001b[0m 54.65   \u001b[0m | \u001b[0m 81.26   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m-2.827   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 15.59   \u001b[0m | \u001b[0m 76.52   \u001b[0m | \u001b[0m 92.61   \u001b[0m | \u001b[0m 37.43   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m-3.068   \u001b[0m | \u001b[0m 17.54   \u001b[0m | \u001b[0m 0.04911 \u001b[0m | \u001b[0m 50.76   \u001b[0m | \u001b[0m 51.52   \u001b[0m | \u001b[0m 81.78   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m-2.862   \u001b[0m | \u001b[0m 5.171   \u001b[0m | \u001b[0m 0.9501  \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 49.32   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 13.8    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m-2.906   \u001b[0m | \u001b[0m 13.61   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 50.61   \u001b[0m | \u001b[0m 75.63   \u001b[0m | \u001b[0m 85.06   \u001b[0m | \u001b[0m 20.71   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m-2.791   \u001b[0m | \u001b[0m 33.64   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 50.53   \u001b[0m | \u001b[0m 78.89   \u001b[0m | \u001b[0m 56.23   \u001b[0m | \u001b[0m 12.33   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m-2.848   \u001b[0m | \u001b[0m 13.57   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 36.06   \u001b[0m | \u001b[0m 88.74   \u001b[0m | \u001b[0m 71.07   \u001b[0m | \u001b[0m 19.59   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m-2.792   \u001b[0m | \u001b[0m 31.73   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 56.5    \u001b[0m | \u001b[0m 57.74   \u001b[0m | \u001b[0m 63.71   \u001b[0m | \u001b[0m 11.71   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.458   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 32.77   \u001b[0m | \u001b[0m 84.31   \u001b[0m | \u001b[0m 80.62   \u001b[0m | \u001b[0m 39.29   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m-2.816   \u001b[0m | \u001b[0m 8.258   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 37.91   \u001b[0m | \u001b[0m 86.27   \u001b[0m | \u001b[0m 56.03   \u001b[0m | \u001b[0m 39.73   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m-2.818   \u001b[0m | \u001b[0m 9.864   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 60.13   \u001b[0m | \u001b[0m 51.25   \u001b[0m | \u001b[0m 24.06   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m-2.809   \u001b[0m | \u001b[0m 14.19   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 64.66   \u001b[0m | \u001b[0m 64.56   \u001b[0m | \u001b[0m 5.851   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m-11.79   \u001b[0m | \u001b[0m 11.62   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 27.54   \u001b[0m | \u001b[0m 110.7   \u001b[0m | \u001b[0m 21.66   \u001b[0m | \u001b[0m 20.65   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m-2.798   \u001b[0m | \u001b[0m 16.67   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 33.51   \u001b[0m | \u001b[0m 94.49   \u001b[0m | \u001b[0m 41.99   \u001b[0m | \u001b[0m 33.68   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m-2.783   \u001b[0m | \u001b[0m 22.8    \u001b[0m | \u001b[0m 0.8922  \u001b[0m | \u001b[0m 45.43   \u001b[0m | \u001b[0m 101.9   \u001b[0m | \u001b[0m 40.73   \u001b[0m | \u001b[0m 21.75   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m-2.817   \u001b[0m | \u001b[0m 26.92   \u001b[0m | \u001b[0m 0.9746  \u001b[0m | \u001b[0m 33.6    \u001b[0m | \u001b[0m 85.58   \u001b[0m | \u001b[0m 30.89   \u001b[0m | \u001b[0m 28.72   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m-11.79   \u001b[0m | \u001b[0m 10.05   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 47.09   \u001b[0m | \u001b[0m 104.4   \u001b[0m | \u001b[0m 40.94   \u001b[0m | \u001b[0m 30.58   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Warnings due to some current issue with xgboost incompatibility with pandas deprecation\n",
    "# Fix will be for upcoming xgboost version 1.0.0, but latest version is only 0.90\n",
    "# See https://github.com/dmlc/xgboost/issues/4300\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Fixed random state due to xgboost hyper param combination throwing a result for NaN\n",
    "optimizer_xgb = BayesianOptimization(cv_mse_xgb, xgb_params, random_state = 1)\n",
    "optimizer_xgb.maximize(n_iter = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.23"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(1.23, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn Random Forest\n",
    "lgbm_params = {'n_estimators':(10, 150), 'max_depth':(10, 200), 'learning_rate':(0.001, 1),\n",
    "               'reg_alpha':(0, 100), 'reg_lambda':(0, 100)}\n",
    "\n",
    "def cv_mse_lgbm(n_estimators, max_depth, learning_rate, reg_alpha, reg_lambda):\n",
    "    \"\"\" \n",
    "    Performs cross validation for Random Forest Regressor. To be used for Bayesian optimiser maximizer function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : float\n",
    "        Number of estimators\n",
    "    max_depth : float\n",
    "        Max depth of trees\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    reg_alpha : float\n",
    "        L1 regularisation\n",
    "    reg_lambda : float\n",
    "        L2 regularisation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cross validation score based on negative mean squared error.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Convert chosen hyperparams to discrete integer\n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    "    \n",
    "    estimator = lgbm.LGBMRegressor(n_estimators = n_estimators, \n",
    "                                 max_depth = max_depth, \n",
    "                                 learning_rate = learning_rate, \n",
    "                                 reg_alpha = reg_alpha, \n",
    "                                 reg_lambda = reg_lambda)\n",
    "    \n",
    "    # Note that neg_mean_squared_error is opposite, thus a negative sign is added for Minimisation Optimisation via BayesOpt\n",
    "    return cross_validate(estimator, X_train_trans, y_train, cv = 10, scoring = \"neg_root_mean_squared_error\")[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | max_depth | n_esti... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.727   \u001b[0m | \u001b[0m 0.4176  \u001b[0m | \u001b[0m 74.83   \u001b[0m | \u001b[0m 10.02   \u001b[0m | \u001b[0m 30.23   \u001b[0m | \u001b[0m 14.68   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-2.749   \u001b[0m | \u001b[0m 0.09325 \u001b[0m | \u001b[0m 26.76   \u001b[0m | \u001b[0m 58.38   \u001b[0m | \u001b[0m 39.68   \u001b[0m | \u001b[0m 53.88   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-2.809   \u001b[0m | \u001b[0m 0.4198  \u001b[0m | \u001b[0m 71.67   \u001b[0m | \u001b[0m 38.62   \u001b[0m | \u001b[0m 87.81   \u001b[0m | \u001b[0m 2.739   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-2.802   \u001b[0m | \u001b[0m 0.6708  \u001b[0m | \u001b[0m 47.56   \u001b[0m | \u001b[0m 88.22   \u001b[0m | \u001b[0m 14.04   \u001b[0m | \u001b[0m 19.81   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-2.79    \u001b[0m | \u001b[0m 0.8009  \u001b[0m | \u001b[0m 97.14   \u001b[0m | \u001b[0m 53.88   \u001b[0m | \u001b[0m 69.23   \u001b[0m | \u001b[0m 87.64   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-2.769   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 4.7e-11 \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-3.167   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-3.19    \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-3.136   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-3.195   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-3.558   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-3.003   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.83    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 56.1    \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 96.48   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 44.26   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-2.962   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 73.19   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m-2.768   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 46.95   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 56.19   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m-3.107   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 31.08   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.776e-1\u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 84.97   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m-2.795   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 59.76   \u001b[0m | \u001b[0m 117.2   \u001b[0m | \u001b[0m 60.7    \u001b[0m | \u001b[0m 50.21   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m-2.794   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 48.32   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 62.75   \u001b[0m | \u001b[0m 46.16   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m-2.842   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 49.54   \u001b[0m | \u001b[0m 37.4    \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m-3.185   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 67.76   \u001b[0m | \u001b[0m 66.97   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m-3.197   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 45.93   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m-3.026   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 58.01   \u001b[0m | \u001b[0m 43.57   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 58.73   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m-2.787   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 74.21   \u001b[0m | \u001b[0m 44.02   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m-2.832   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 50.4    \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m-3.297   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m-2.767   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 52.45   \u001b[0m | \u001b[0m 51.81   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m-2.83    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 60.16   \u001b[0m | \u001b[0m 95.85   \u001b[0m | \u001b[0m 63.19   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m-2.811   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 107.1   \u001b[0m | \u001b[0m 38.24   \u001b[0m | \u001b[0m 44.58   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m-2.763   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 114.4   \u001b[0m | \u001b[0m 51.74   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m-3.165   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 86.91   \u001b[0m | \u001b[0m 49.59   \u001b[0m | \u001b[0m 46.72   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m-2.734   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 60.26   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 48.86   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m-2.941   \u001b[0m | \u001b[0m 0.06052 \u001b[0m | \u001b[0m 76.04   \u001b[0m | \u001b[0m 10.55   \u001b[0m | \u001b[0m 28.35   \u001b[0m | \u001b[0m 13.32   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m-2.835   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 45.59   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 55.79   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 80.31   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 63.9    \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m-2.964   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 80.74   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m-2.77    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 97.87   \u001b[0m | \u001b[0m 56.93   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m-3.154   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 43.31   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 43.84   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 81.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m-3.589   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 99.55   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m-3.042   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 47.68   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 53.23   \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m-2.756   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 54.05   \u001b[0m | \u001b[0m 97.19   \u001b[0m | \u001b[0m 26.7    \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m-2.744   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 49.66   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 24.61   \u001b[0m | \u001b[0m 43.19   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m-2.785   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 80.01   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 61.6    \u001b[0m | \u001b[0m 20.37   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m-3.554   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 82.37   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m-2.822   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 31.94   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 45.82   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m-2.772   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 49.98   \u001b[0m | \u001b[0m 67.47   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m-2.781   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 46.99   \u001b[0m | \u001b[0m 56.46   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m-2.822   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 61.52   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 59.48   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m-2.78    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 60.06   \u001b[0m | \u001b[0m 48.24   \u001b[0m | \u001b[0m 47.94   \u001b[0m | \u001b[0m 27.76   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m-2.759   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 52.47   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m-2.784   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 60.63   \u001b[0m | \u001b[0m 48.62   \u001b[0m | \u001b[0m 31.46   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 57.58   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 51.55   \u001b[0m | \u001b[0m 71.52   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 32.39   \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 55.19   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m-3.148   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 54.79   \u001b[0m | \u001b[0m 121.5   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 103.1   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m-3.089   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 45.32   \u001b[0m | \u001b[0m 97.09   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 64.47   \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m-3.057   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 48.13   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 46.93   \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m-2.825   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 50.41   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 36.43   \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m-2.818   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 123.7   \u001b[0m | \u001b[0m 58.72   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m-2.764   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 49.14   \u001b[0m | \u001b[0m 23.09   \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m-2.815   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 46.42   \u001b[0m | \u001b[0m 77.17   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m-2.793   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 53.27   \u001b[0m | \u001b[0m 118.3   \u001b[0m | \u001b[0m 81.93   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m-2.798   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 118.0   \u001b[0m | \u001b[0m 76.32   \u001b[0m | \u001b[0m 27.49   \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m-2.769   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 50.64   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m-2.798   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 71.69   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 39.24   \u001b[0m | \u001b[0m 37.06   \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m-2.842   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 56.66   \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m-2.803   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 71.28   \u001b[0m | \u001b[0m 68.35   \u001b[0m | \u001b[0m 34.33   \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m-2.828   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 57.8    \u001b[0m | \u001b[0m 70.28   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m-3.135   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 46.01   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 43.79   \u001b[0m | \u001b[0m 40.31   \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m-3.155   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 46.94   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m-3.158   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 57.6    \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m-2.995   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 54.09   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m-3.162   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 47.69   \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 45.3    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 32.68   \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m-2.782   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 49.49   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 36.25   \u001b[0m | \u001b[0m 18.54   \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m-2.775   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 46.25   \u001b[0m | \u001b[0m 79.96   \u001b[0m | \u001b[0m 62.82   \u001b[0m | \u001b[0m 75.34   \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m-2.806   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 72.57   \u001b[0m | \u001b[0m 59.79   \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m-3.138   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 121.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 59.29   \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 60.42   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 102.5   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m-2.735   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 70.62   \u001b[0m | \u001b[0m 40.11   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m-2.793   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 150.0   \u001b[0m | \u001b[0m 65.29   \u001b[0m | \u001b[0m 21.38   \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m-2.742   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 57.93   \u001b[0m | \u001b[0m 49.75   \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m-2.827   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 114.0   \u001b[0m | \u001b[0m 60.19   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m-2.83    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 54.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m-2.831   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 38.96   \u001b[0m | \u001b[0m 50.68   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m-2.919   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 35.41   \u001b[0m | \u001b[0m 43.62   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m-2.796   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 52.84   \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m-2.792   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 40.01   \u001b[0m | \u001b[0m 36.55   \u001b[0m | \u001b[0m 82.27   \u001b[0m | \u001b[0m 72.3    \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m-2.823   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 125.9   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 61.12   \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m-2.83    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 75.36   \u001b[0m | \u001b[0m 120.5   \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 72.08   \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m-2.783   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 69.72   \u001b[0m | \u001b[0m 124.8   \u001b[0m | \u001b[0m 34.17   \u001b[0m | \u001b[0m 78.04   \u001b[0m |\n",
      "| \u001b[0m 103     \u001b[0m | \u001b[0m-2.772   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 41.46   \u001b[0m | \u001b[0m 85.41   \u001b[0m | \u001b[0m 47.52   \u001b[0m | \u001b[0m 28.87   \u001b[0m |\n",
      "| \u001b[0m 104     \u001b[0m | \u001b[0m-2.859   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 57.93   \u001b[0m | \u001b[0m 119.5   \u001b[0m | \u001b[0m 27.61   \u001b[0m | \u001b[0m 0.0     \u001b[0m |\n",
      "| \u001b[0m 105     \u001b[0m | \u001b[0m-2.77    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 27.07   \u001b[0m | \u001b[0m 119.2   \u001b[0m | \u001b[0m 62.8    \u001b[0m | \u001b[0m 72.49   \u001b[0m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fixed random state due to xgboost hyper param combination throwing a result for NaN\n",
    "optimizer_lgbm = BayesianOptimization(cv_mse_lgbm, lgbm_params, random_state = 1)\n",
    "optimizer_lgbm.maximize(n_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lm_lasso</th>\n",
       "      <td>2.756544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lm_ridge</th>\n",
       "      <td>2.735042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest</th>\n",
       "      <td>2.627075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>2.727759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>2.726576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cv_score\n",
       "lm_lasso      2.756544\n",
       "lm_ridge      2.735042\n",
       "randomforest  2.627075\n",
       "xgb           2.727759\n",
       "lgbm          2.726576"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the best models\n",
    "cv_rmse = [-optimizer_lmlasso.max['target'], \n",
    "             -optimizer_lmridge.max['target'], \n",
    "             -optimizer_rf.max['target'],\n",
    "             -optimizer_xgb.max['target'],\n",
    "             -optimizer_lgbm.max['target']]\n",
    "\n",
    "models = [\"lm_lasso\", \"lm_ridge\", \"randomforest\", \"xgb\", \"lgbm\"]\n",
    "\n",
    "cv_df = pd.DataFrame(cv_rmse, index = models, columns = [\"cv_score\"])\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting discrete hyperparam for certain models\n",
    "rf_hyperparam = optimizer_rf.max['params']\n",
    "rf_hyperparam['max_depth'] = int(rf_hyperparam['max_depth'])\n",
    "rf_hyperparam['max_features'] = int(rf_hyperparam['max_features'])\n",
    "rf_hyperparam['n_estimators'] = int(rf_hyperparam['n_estimators'])\n",
    "\n",
    "xgb_hyperparam = optimizer_xgb.max['params']\n",
    "xgb_hyperparam['max_depth'] = int(xgb_hyperparam['max_depth'])\n",
    "xgb_hyperparam['n_estimators'] = int(xgb_hyperparam['n_estimators'])\n",
    "\n",
    "lgbm_hyperparam = optimizer_lgbm.max['params']\n",
    "lgbm_hyperparam['max_depth'] = int(lgbm_hyperparam['max_depth'])\n",
    "lgbm_hyperparam['n_estimators'] = int(lgbm_hyperparam['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as Series for writing to csv\n",
    "lmlasso_hyperparam = pd.Series(optimizer_lmlasso.max['params'])\n",
    "\n",
    "lmridge_hyperparam = pd.Series(optimizer_lmridge.max['params'])\n",
    "\n",
    "rf_hyperparam = pd.Series(rf_hyperparam)\n",
    "\n",
    "xgb_hyperparam = pd.Series(xgb_hyperparam)\n",
    "\n",
    "lgbm_hyperparam = pd.Series(lgbm_hyperparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to csv\n",
    "cv_df.to_csv(\"../data/output/cv_results.csv\")\n",
    "\n",
    "lmlasso_hyperparam.to_csv(\"../data/output/lmlasso_hyperparam.csv\")\n",
    "lmridge_hyperparam.to_csv(\"../data/output/lmridge_hyperparam.csv\")\n",
    "rf_hyperparam.to_csv(\"../data/output/rf_hyperparam.csv\")\n",
    "xgb_hyperparam.to_csv(\"../data/output/xgb_hyperparam.csv\")\n",
    "lgbm_hyperparam.to_csv(\"../data/output/lgbm_hyperparam.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "test_data  = pd.read_csv(\"../data/processed/test.csv\")\n",
    "\n",
    "X_test = test_data.drop([\"G3\"], axis = 1)\n",
    "y_test = test_data[\"G3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "X_test_trans = pd.DataFrame(preprocessor.fit_transform(X_test),\n",
    "                            index = X_test.index,\n",
    "                             columns = (list(numeric_features) +\n",
    "                                       list(preprocessor.named_transformers_['ohe'].get_feature_names(categorical_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.0039689085736815954, copy_X=True, fit_intercept=True,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lasso = Lasso().set_params(**lmlasso_hyperparam)\n",
    "best_lasso.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=61.716008089193764, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ridge = Ridge().set_params(**lmridge_hyperparam)\n",
    "best_ridge.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=63, max_features=7, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=114, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf = RandomForestRegressor().set_params(**rf_hyperparam)\n",
    "best_rf.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:24:29] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=8.491520978228445,\n",
       "             importance_type='gain', learning_rate=0.8781425034294131,\n",
       "             max_delta_step=0, max_depth=18, min_child_weight=1, missing=None,\n",
       "             n_estimators=68, n_jobs=1, nthread=None, objective='reg:linear',\n",
       "             random_state=0, reg_alpha=95.7889530150502,\n",
       "             reg_lambda=53.316528497301704, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=0.6918771139504734, verbosity=1)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb = XGBRegressor().set_params(**xgb_hyperparam)\n",
    "best_xgb.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.41760498269787144,\n",
       "              max_depth=74, min_child_samples=20, min_child_weight=0.001,\n",
       "              min_split_gain=0.0, n_estimators=10, n_jobs=-1, num_leaves=31,\n",
       "              objective=None, random_state=None, reg_alpha=30.233257263183976,\n",
       "              reg_lambda=14.675589081711305, silent=True, subsample=1.0,\n",
       "              subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lgbm = lgbm.LGBMRegressor().set_params(**lgbm_hyperparam)\n",
    "best_lgbm.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rmse = []\n",
    "test_rmse.append(np.sqrt(mean_squared_error(y_test, best_lasso.predict(X_test_trans))))\n",
    "test_rmse.append(np.sqrt(mean_squared_error(y_test, best_ridge.predict(X_test_trans))))\n",
    "test_rmse.append(np.sqrt(mean_squared_error(y_test, best_rf.predict(X_test_trans))))\n",
    "test_rmse.append(np.sqrt(mean_squared_error(y_test, best_xgb.predict(X_test_trans))))\n",
    "test_rmse.append(np.sqrt(mean_squared_error(y_test, best_lgbm.predict(X_test_trans))))\n",
    "\n",
    "# Convert to Dataframe\n",
    "test_rmse = pd.DataFrame(test_rmse, index= models, columns = [\"test_rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lm_lasso</th>\n",
       "      <td>2.483822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lm_ridge</th>\n",
       "      <td>2.481183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>randomforest</th>\n",
       "      <td>2.458307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>2.510746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>2.577540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              test_rmse\n",
       "lm_lasso       2.483822\n",
       "lm_ridge       2.481183\n",
       "randomforest   2.458307\n",
       "xgb            2.510746\n",
       "lgbm           2.577540"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.062015503875969"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_rmse    randomforest\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Best model\n",
    "print(test_rmse.idxmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance = pd.DataFrame(best_rf.feature_importances_, index = X_train_trans.columns, columns = [\"Importance\"])\n",
    "feat_importance = feat_importance.sort_values(by = \"Importance\", ascending = False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>failures</td>\n",
       "      <td>0.097314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>absences</td>\n",
       "      <td>0.055374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>school_MS</td>\n",
       "      <td>0.051027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age</td>\n",
       "      <td>0.049253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goout</td>\n",
       "      <td>0.047025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Walc</td>\n",
       "      <td>0.046958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Medu</td>\n",
       "      <td>0.046369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>higher_yes</td>\n",
       "      <td>0.045723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>freetime</td>\n",
       "      <td>0.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>studytime</td>\n",
       "      <td>0.038511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fedu</td>\n",
       "      <td>0.038441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>health</td>\n",
       "      <td>0.036242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dalc</td>\n",
       "      <td>0.034681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>famrel</td>\n",
       "      <td>0.034534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>address_U</td>\n",
       "      <td>0.025794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traveltime</td>\n",
       "      <td>0.024871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>internet_yes</td>\n",
       "      <td>0.020803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sex_M</td>\n",
       "      <td>0.020526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>activities_yes</td>\n",
       "      <td>0.018878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>famsup_yes</td>\n",
       "      <td>0.018168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>reason_reputation</td>\n",
       "      <td>0.016779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nursery_yes</td>\n",
       "      <td>0.015634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>romantic_yes</td>\n",
       "      <td>0.015108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>schoolsup_yes</td>\n",
       "      <td>0.015024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>reason_other</td>\n",
       "      <td>0.013771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>reason_home</td>\n",
       "      <td>0.013699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Fjob_services</td>\n",
       "      <td>0.012911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fjob_teacher</td>\n",
       "      <td>0.012910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>famsize_LE3</td>\n",
       "      <td>0.012840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>guardian_mother</td>\n",
       "      <td>0.011971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mjob_other</td>\n",
       "      <td>0.011312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Fjob_other</td>\n",
       "      <td>0.011247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mjob_teacher</td>\n",
       "      <td>0.009969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Pstatus_T</td>\n",
       "      <td>0.009755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Mjob_services</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Mjob_health</td>\n",
       "      <td>0.007235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>paid_yes</td>\n",
       "      <td>0.005014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>guardian_other</td>\n",
       "      <td>0.003965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Fjob_health</td>\n",
       "      <td>0.002565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index  Importance\n",
       "0            failures    0.097314\n",
       "1            absences    0.055374\n",
       "2           school_MS    0.051027\n",
       "3                 age    0.049253\n",
       "4               goout    0.047025\n",
       "5                Walc    0.046958\n",
       "6                Medu    0.046369\n",
       "7          higher_yes    0.045723\n",
       "8            freetime    0.038600\n",
       "9           studytime    0.038511\n",
       "10               Fedu    0.038441\n",
       "11             health    0.036242\n",
       "12               Dalc    0.034681\n",
       "13             famrel    0.034534\n",
       "14          address_U    0.025794\n",
       "15         traveltime    0.024871\n",
       "16       internet_yes    0.020803\n",
       "17              sex_M    0.020526\n",
       "18     activities_yes    0.018878\n",
       "19         famsup_yes    0.018168\n",
       "20  reason_reputation    0.016779\n",
       "21        nursery_yes    0.015634\n",
       "22       romantic_yes    0.015108\n",
       "23      schoolsup_yes    0.015024\n",
       "24       reason_other    0.013771\n",
       "25        reason_home    0.013699\n",
       "26      Fjob_services    0.012911\n",
       "27       Fjob_teacher    0.012910\n",
       "28        famsize_LE3    0.012840\n",
       "29    guardian_mother    0.011971\n",
       "30         Mjob_other    0.011312\n",
       "31         Fjob_other    0.011247\n",
       "32       Mjob_teacher    0.009969\n",
       "33          Pstatus_T    0.009755\n",
       "34      Mjob_services    0.009200\n",
       "35        Mjob_health    0.007235\n",
       "36           paid_yes    0.005014\n",
       "37     guardian_other    0.003965\n",
       "38        Fjob_health    0.002565"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "ename": "SchemaValidationError",
     "evalue": "Invalid specification\n\n        altair.vegalite.v3.schema.core.SortField, validating 'additionalProperties'\n\n        Additional properties are not allowed ('op' was unexpected)\n        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaValidationError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-99680d1f5545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maltair\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m c1 = alt.Chart(feat_importance).mark_bar(color='black').encode(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSortField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'descending'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Importance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Importance'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/altair/vegalite/v3/schema/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, field, order, **kwds)\u001b[0m\n\u001b[1;32m  12776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12777\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12778\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSortField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  12779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/altair/utils/schemapi.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG_MODE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_is_valid_at_instantiation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/altair/utils/schemapi.py\u001b[0m in \u001b[0;36mto_dict\u001b[0;34m(self, validate, ignore, context)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mjsonschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSchemaValidationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSchemaValidationError\u001b[0m: Invalid specification\n\n        altair.vegalite.v3.schema.core.SortField, validating 'additionalProperties'\n\n        Additional properties are not allowed ('op' was unexpected)\n        "
     ]
    }
   ],
   "source": [
    "# https://github.com/nipunbatra/50-ggplot-python/blob/master/Altair/DivergingLollipop.ipynb\n",
    "import altair as alt\n",
    "from altair import *\n",
    "c1 = alt.Chart(feat_importance).mark_bar(color='black').encode(\n",
    "    y=alt.Y('index', sort=SortField(op='mean', order='descending', field='Importance')),\n",
    "    x=alt.X('Importance' )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = Chart(mtcars).mark_circle(color='black', size=400).encode(y=Y('model', sort=SortField(op='mean', order='descending', field='mpg_z')),\n",
    "                                x=X('mpg_z' ), text='mpg_z').transform_data(\n",
    "   calculate=[Formula('Performance', expr.where(expr.df.mpg_z < mean_mpg_z,'Below average','Above average'))],\n",
    "   \n",
    ").configure_cell(height=100, width=400).configure_scale(bandSize=22)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
